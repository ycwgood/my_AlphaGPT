"""Main entry point for AlphaGPT - Part 1: Imports and initialization"""

import warnings
import numpy as np
import pandas as pd
import logging
import os

from alphagpt.config import Config
from alphagpt.utils import setup_logger, set_seed, create_run_output_dir
from alphagpt.data import DataLoader, FeatureEngineer
from alphagpt.factor import FactorGenerator, FactorCalculator
from alphagpt.backtest import BacktestEngine
from alphagpt.visualization import ResultPlotter

warnings.filterwarnings('ignore')


def load_and_merge_multiple_stocks(config, logger):
    """
    åŠ è½½å¹¶åˆå¹¶å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®

    Args:
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—å¯¹è±¡

    Returns:
        åˆå¹¶åçš„DataFrame
    """
    print(f"\næ­£åœ¨åŠ è½½ {len(config.codes)} ä¸ªæ ‡çš„çš„æ•°æ®...")

    # æ‰¹é‡åŠ è½½å¤šä¸ªè‚¡ç¥¨æ•°æ®
    stock_data_dict = DataLoader.get_multiple_price_data(
        codes=config.codes,
        start=config.start_date,
        end=config.test_end
    )

    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    all_dfs = []
    for code, df in stock_data_dict.items():
        df = df.copy()
        df['code'] = code  # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
        all_dfs.append(df)

    df_merged = pd.concat(all_dfs, ignore_index=True)
    logger.info(f"åˆå¹¶åæ•°æ®æ€»è¡Œæ•°: {len(df_merged)}")

    return df_merged


def main():
    """Main function to run AlphaGPT system"""
    print("=" * 60)
    print("AlphaGPT é‡åŒ–å› å­ç”Ÿæˆä¸å›æµ‹ç³»ç»Ÿ")
    print("=" * 60)

    # Load configuration from YAML file
    config = Config.from_yaml('config.yaml')

    # Setup logger
    logger = setup_logger(__name__, config.log_level, config.log_file)

    # è®°å½•é…ç½®ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("AlphaGPT é‡åŒ–å› å­ç”Ÿæˆä¸å›æµ‹ç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 60)
    logger.info(f"é…ç½®ä¿¡æ¯:")
    logger.info(f"  - è‚¡ç¥¨ä»£ç : {config.codes}")
    logger.info(f"  - æ•°æ®æ—¥æœŸèŒƒå›´: {config.start_date} ~ {config.test_end}")
    logger.info(f"  - è®­ç»ƒé›†æˆªæ­¢: {config.end_date}")
    logger.info(f"  - å› å­ç”Ÿæˆæ•°é‡: {config.num_factors}")
    logger.info(f"  - æœ€å¤§åºåˆ—é•¿åº¦: {config.max_seq_len}")
    logger.info(f"  - æœ€å°æ“ä½œæ•°: {config.min_op_count}")
    logger.info(f"  - APIæ¨¡å‹: {config.model}")
    logger.info(f"  - éšæœºç§å­: {config.random_seed}")

    # Set random seed
    set_seed(config.random_seed)

    # Create output directory with subfolder structure: results/YYYY-MM-DD/stock_codes/
    output_dir = create_run_output_dir(
        base_dir="results",
        codes=config.codes
    )
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

    # Load data - æ”¯æŒå¤šæ ‡çš„
    logger.info("å¼€å§‹åŠ è½½æ•°æ®...")
    df_all = load_and_merge_multiple_stocks(config, logger)

    if df_all is None or len(df_all) == 0:
        logger.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    # Split data
    split_date = pd.to_datetime(config.end_date).date()
    train_df_raw = df_all[df_all.trade_date <= split_date].copy()
    test_df_raw = df_all[df_all.trade_date > split_date].copy()
    logger.info(f"æ•°æ®åˆ’åˆ†å®Œæˆ: è®­ç»ƒé›† {len(train_df_raw)} è¡Œ, æµ‹è¯•é›† {len(test_df_raw)} è¡Œ")

    # Feature engineering
    logger.info("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
    print("è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    train_df, features = FeatureEngineer.create_features(train_df_raw)
    test_df, _ = FeatureEngineer.create_features(test_df_raw)

    features = [f for f in features if f in train_df.columns and f in test_df.columns]
    train_df = train_df[['trade_date', 'close'] + features].copy()
    test_df = test_df[['trade_date', 'close'] + features].copy()

    logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: ç”Ÿæˆ {len(features)} ä¸ªç‰¹å¾")
    logger.info(f"è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬ ({train_df['trade_date'].min()} ~ {train_df['trade_date'].max()})")
    logger.info(f"æµ‹è¯•é›†: {len(test_df)} æ ·æœ¬ ({test_df['trade_date'].min()} ~ {test_df['trade_date'].max()})")

    print(f"è®­ç»ƒé›†: {len(train_df)} ä¸ªæ ·æœ¬ ({train_df['trade_date'].min()} åˆ° {train_df['trade_date'].max()})")
    print(f"æµ‹è¯•é›†: {len(test_df)} ä¸ªæ ·æœ¬ ({test_df['trade_date'].min()} åˆ° {test_df['trade_date'].max()})")
    print(f"ç‰¹å¾: {', '.join(features)}")

    # Calculate target returns
    train_returns = np.zeros(len(train_df))
    train_returns[:-1] = train_df['close'].values[1:] / train_df['close'].values[:-1] - 1
    train_returns = np.clip(train_returns, -0.1, 0.1)

    test_returns = np.zeros(len(test_df))
    test_returns[:-1] = test_df['close'].values[1:] / test_df['close'].values[:-1] - 1
    test_returns = np.clip(test_returns, -0.1, 0.1)

    # Initialize factor generator
    print("\nåˆå§‹åŒ–Geminiå› å­ç”Ÿæˆå™¨...")
    logger.info("åˆå§‹åŒ–å› å­ç”Ÿæˆå™¨...")
    factor_generator = FactorGenerator(
        api_key=config.gemini_api_key,
        model=config.gemini_model,
        proxy=config.gemini_proxy
    )

    # Generate factors
    print(f"\nç”Ÿæˆé‡åŒ–å› å­...")
    logger.info(f"å¼€å§‹ç”Ÿæˆ {config.num_factors} ä¸ªå› å­...")
    factor_exprs = factor_generator.generate_factors_with_gemini(
        features,
        max_seq_len=config.max_seq_len,
        min_op_count=config.min_op_count,
        num_factors=config.num_factors
    )

    if not factor_exprs:
        print("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå› å­ï¼Œä½¿ç”¨é»˜è®¤å› å­")
        logger.warning("APIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå› å­ï¼Œä½¿ç”¨é»˜è®¤å› å­")
        factor_exprs = [
            "ret_norm + vol_chg_norm * trend_norm",
            "abs(ret5_norm) - vol_chg_norm",
            "mom5_norm * vol_chg_norm",
            "ret_norm / (trend_norm + 1e-6)",
            "ret_norm - vol_chg_norm"
        ]

    print(f"\nç”Ÿæˆçš„å› å­è¡¨è¾¾å¼ ({len(factor_exprs)} ä¸ª):")
    logger.info(f"æˆåŠŸç”Ÿæˆ {len(factor_exprs)} ä¸ªå› å­è¡¨è¾¾å¼:")
    for i, expr in enumerate(factor_exprs, 1):
        print(f"{i}. {expr}")
        logger.info(f"  Factor_{i}: {expr}")

    # Prepare feature data
    train_feature_data = {f: train_df[f].values for f in features}
    test_feature_data = {f: test_df[f].values for f in features}

    # Initialize calculator
    calculator = FactorCalculator()

    # Evaluate and backtest all factors
    print(f"\nè¯„ä¼°å’Œå›æµ‹å› å­...")
    logger.info("å¼€å§‹è¯„ä¼°å’Œå›æµ‹å› å­...")
    results = {}
    best_sharpe = -np.inf
    best_factor_name = None

    for i, expr in enumerate(factor_exprs, 1):
        factor_name = f"Factor_{i}"
        print(f"\n[{i}/{len(factor_exprs)}] è¯„ä¼° {factor_name}")
        print(f"è¡¨è¾¾å¼: {expr}")
        logger.info(f"è¯„ä¼° {factor_name}: {expr}")

        # Evaluate on training set
        factor_values_train = calculator.calculate_factor_value(expr, train_feature_data)

        if factor_values_train is not None and len(factor_values_train) == len(train_df):
            train_sharpe = calculator.calculate_sharpe(factor_values_train, train_returns)
            print(f"è®­ç»ƒé›†å¤æ™®æ¯”ç‡: {train_sharpe:.3f}")
            logger.info(f"  è®­ç»ƒé›†å¤æ™®æ¯”ç‡: {train_sharpe:.3f}")

            # Calculate factor values on test set
            factor_values_test = calculator.calculate_factor_value(expr, test_feature_data)

            if factor_values_test is not None and len(factor_values_test) == len(test_df):
                # Backtest strategy
                result = BacktestEngine.backtest_strategy(
                    factor_values_test, test_df, test_returns, factor_name
                )

                if result is not None:
                    results[factor_name] = result
                    print(f"æµ‹è¯•é›†å¤æ™®æ¯”ç‡: {result['sharpe']:.3f}")
                    logger.info(f"  æµ‹è¯•é›†å¤æ™®æ¯”ç‡: {result['sharpe']:.3f}, å¹´åŒ–æ”¶ç›Š: {result['annual_return']:.2%}, æœ€å¤§å›æ’¤: {result['max_drawdown']:.2%}")

                    # Update best factor
                    if result['sharpe'] > best_sharpe:
                        best_sharpe = result['sharpe']
                        best_factor_name = factor_name
                else:
                    print(f"æµ‹è¯•é›†å›æµ‹å¤±è´¥")
                    logger.warning(f"  {factor_name} æµ‹è¯•é›†å›æµ‹å¤±è´¥")
            else:
                print(f"æµ‹è¯•é›†å› å­è®¡ç®—å¤±è´¥")
                logger.warning(f"  {factor_name} æµ‹è¯•é›†å› å­è®¡ç®—å¤±è´¥")
        else:
            print(f"è®­ç»ƒé›†å› å­è®¡ç®—å¤±è´¥")
            logger.warning(f"  {factor_name} è®­ç»ƒé›†å› å­è®¡ç®—å¤±è´¥")

    # Output results summary
    print(f"\n" + "=" * 60)
    print("ç»“æœæ±‡æ€»")
    print("=" * 60)
    logger.info("=" * 60)
    logger.info("ç»“æœæ±‡æ€»")
    logger.info("=" * 60)

    if best_factor_name and best_factor_name in results:
        best_result = results[best_factor_name]
        # å®‰å…¨è§£æå› å­ç´¢å¼•
        try:
            best_expr_idx = int(best_factor_name.split('_')[1]) - 1
            best_expr = factor_exprs[best_expr_idx] if 0 <= best_expr_idx < len(factor_exprs) else "N/A"
        except (IndexError, ValueError):
            best_expr = "N/A"

        print(f"ğŸ† æœ€ä½³å› å­: {best_factor_name}")
        print(f"   è¡¨è¾¾å¼: {best_expr}")
        print(f"\nğŸ“Š æµ‹è¯•é›†ç»©æ•ˆ:")
        print(f"   å¤æ™®æ¯”ç‡: {best_result['sharpe']:.3f}")
        print(f"   å¹´åŒ–æ”¶ç›Šç‡: {best_result['annual_return']:.2%}")
        print(f"   å¹´åŒ–æ³¢åŠ¨ç‡: {best_result['annual_vol']:.2%}")
        print(f"   æœ€å¤§å›æ’¤: {best_result['max_drawdown']:.2%}")
        print(f"   æ€»æ”¶ç›Šç‡: {best_result['total_return']:.2%}")
        print(f"   ä¿¡æ¯æ¯”ç‡: {best_result['info_ratio']:.2f}")
        print(f"   èƒœç‡: {best_result['win_rate']:.1f}%")
        print(f"   å¡ç›æ¯”ç‡: {best_result['calmar']:.2f}")

        # è®°å½•æœ€ä½³å› å­åˆ°æ—¥å¿—
        logger.info(f"æœ€ä½³å› å­: {best_factor_name}")
        logger.info(f"è¡¨è¾¾å¼: {best_expr}")
        logger.info(f"æµ‹è¯•é›†ç»©æ•ˆ:")
        logger.info(f"  å¤æ™®æ¯”ç‡: {best_result['sharpe']:.3f}")
        logger.info(f"  å¹´åŒ–æ”¶ç›Šç‡: {best_result['annual_return']:.2%}")
        logger.info(f"  å¹´åŒ–æ³¢åŠ¨ç‡: {best_result['annual_vol']:.2%}")
        logger.info(f"  æœ€å¤§å›æ’¤: {best_result['max_drawdown']:.2%}")
        logger.info(f"  æ€»æ”¶ç›Šç‡: {best_result['total_return']:.2%}")
        logger.info(f"  ä¿¡æ¯æ¯”ç‡: {best_result['info_ratio']:.2f}")
        logger.info(f"  èƒœç‡: {best_result['win_rate']:.1f}%")
        logger.info(f"  å¡ç›æ¯”ç‡: {best_result['calmar']:.2f}")
        logger.info(f"  ä¿¡æ¯ç³»æ•°(IC): {best_result.get('ic', np.nan):.3f}")

        # Calculate factor values for the best factor (for rolling metrics)
        best_expr = factor_exprs[int(best_factor_name.split('_')[1]) - 1]
        best_factor_values = calculator.calculate_factor_value(best_expr, test_feature_data)

        # Plot results with enhanced visualization (using simpler filename)
        ResultPlotter.plot_results(
            results,
            test_df,
            best_factor_name,
            "performance",  # Simplified filename - directory already identifies the run
            output_dir,
            factor_values=best_factor_values,
            test_returns=test_returns
        )

        # Save detailed results (using simpler filename)
        try:
            summary_data = []
            for factor_name, result in results.items():
                if result is not None:
                    expr_idx = int(factor_name.split('_')[1]) - 1
                    expr = factor_exprs[expr_idx] if expr_idx < len(factor_exprs) else "N/A"
                    ic = result.get('ic', np.nan)
                    summary_data.append({
                        'factor_name': factor_name,
                        'expression': expr,
                        'sharpe': result['sharpe'],
                        'ic': ic,
                        'annual_return': result['annual_return'],
                        'annual_vol': result['annual_vol'],
                        'max_drawdown': result['max_drawdown'],
                        'total_return': result['total_return'],
                        'info_ratio': result['info_ratio'],
                        'win_rate': result['win_rate'],
                        'calmar': result['calmar']
                    })

            summary_df = pd.DataFrame(summary_data)
            summary_path = os.path.join(output_dir, 'summary.csv')
            summary_df.to_csv(summary_path, index=False)
            print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {summary_path}")
            logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {summary_path}")

        except Exception as e:
            print(f"ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³å› å­")
        logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³å› å­")

    print(f"\n" + "=" * 60)
    print("ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼")
    print("=" * 60)
    logger.info("ç¨‹åºæ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logging.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {type(e).__name__}: {e}")
        print(f"\nç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        raise
